文本向量化的表征方式

BOW

Distributed Representation

文本表示的问题**anisotropic**

> **2019_How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings**
>
> https://arxiv.org/pdf/1909.00512v1.pdf
>
> 

> **2020_bert-flow**
>
> **On the sentence embeddings from pre-trained language models**
>
> https://arxiv.org/abs/2011.05864
>
> **Hypothesis: Anisotropic embedding space induces poor semantic similarity** 
>
> Approach: Invertible mapping from the BERT embedding space to standard Gaussian latent space.
>
> Method: 
> $$
> Prior: \bold z \sim P_{Z}(\bold z)
> \\
> observed : \bold u
> \\
> latent: \bold z
> \\
> \bold u = f_{\phi}(\bold z)
> \\
> p_U(\bold u) = p_Z(f_{\phi}^{-1}(\bold u))|det\frac{\partial f_{\phi}^{-1}(\bold u)}{\partial \bold u}|
> $$
> Objective:
> $$
> max_{\phi} \mathbb E_{\bold u} = BERT(sentence), \; sentence \in D
> \\
> logp_Z(f_{\phi}^{-1}(\bold u)) + log|det\frac{\partial f_{\phi}^{-1}(\bold u)}{\partial \bold u}|
> $$
> Final output: 
> $$
> f_{\phi}
> $$
> 具体flow的实现方案要还要参考
>
> https://arxiv.org/abs/1807.03039
>
> https://arxiv.org/abs/1908.09257v1



> **2021_bert-whitening**
>
> **Whitening sentence representations for better sementics and faster retrieval**
>
> takeaway:
>
> 1. bert-sent-embs not work, the reason is vec not in **standard orthogonal basis**
> 2. post-processing converts bert-sent-emb vec into **standard orthogonal basis** & reducing vec dimensions(optional)
>
> Hypothesis:
>
> cosine similarity is more suitable for vec from **standard orthogonal basis**
>
> Approach: transform sent-emb vec into **0 value of mean** and **identity matrix of covariance matrix**
>
> Method:
> $$
> \tilde x_i = (x_i-u)W ,\; i \in {1,2,...N} \\
> x_i: sentence \; embedding \\
> \tilde x_i: transformed \; sentence \; embedding
> \\
> u = \frac{1}{N}\sum_{i=1}^{N}x_i
> \\
> original \; covariance \; matrix: \Sigma=\frac{1}{N} \sum_{i=1}^{N}(x_i-u)^T(x_i-u)
> \\
> transformed \; covariance \; matrix: \tilde \Sigma= W^T\Sigma W 
> \\
> assumed \;   \tilde\Sigma =I = W^T\Sigma W 
> \\
> \Sigma = (W^T)^{-1}W^{-1} = (W^{-1})^{T}W^{-1}
> \\
> \Sigma : positive \; definite \; symmetric \; matrix
> \\
> so\; by\; SVD:
> \\
> \Sigma = U\Lambda U^T
> \\ 
> U \; is \; orthogonal \; matrix\\
> \Lambda \; is \; diagonal \; matrix \; and \; diagonal \; elements \; all \; positive
> \\
> So:\\
> let: \; W^{-1} = \sqrt{\Lambda}U^T \\
> W = U\sqrt{\Lambda^{-1}}
> $$
> 





### Application

> Embedding Retrieval System
>
> Paper
>
> **Embedding-based Retrieval in Facebook Search**
>
> https://dl.acm.org/doi/pdf/10.1145/3394486.3403305